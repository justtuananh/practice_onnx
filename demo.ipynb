{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f1a259",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e473ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    return config\n",
    "\n",
    "class Cfg(dict):\n",
    "    def __init__(self, config_dict):\n",
    "        super(Cfg, self).__init__(**config_dict)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config_from_file(fname, base_file='config/base.yml'):\n",
    "        base_config = load_config(base_file)\n",
    "\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        base_config.update(config)\n",
    "\n",
    "        return Cfg(base_config)\n",
    "\n",
    "\n",
    "    def save(self, fname):\n",
    "        with open(fname, 'w') as outfile:\n",
    "            yaml.dump(dict(self), outfile, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b719cc",
   "metadata": {},
   "source": [
    "# CNN Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d04d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacdcc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg(nn.Module):\n",
    "    def __init__(self, name, ss, ks, hidden, pretrained=True, dropout=0.5):\n",
    "        super(Vgg, self).__init__()\n",
    "\n",
    "        if name == 'vgg11_bn':\n",
    "            cnn = models.vgg11_bn(pretrained=pretrained)\n",
    "        elif name == 'vgg19_bn':\n",
    "            cnn = models.vgg19_bn(pretrained=pretrained)\n",
    "\n",
    "        pool_idx = 0\n",
    "\n",
    "        for i, layer in enumerate(cnn.features):\n",
    "            if isinstance(layer, torch.nn.MaxPool2d):\n",
    "                cnn.features[i] = torch.nn.AvgPool2d(kernel_size=ks[pool_idx], stride=ss[pool_idx], padding=0)\n",
    "                pool_idx += 1\n",
    "\n",
    "        self.features = cnn.features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.last_conv_1x1 = nn.Conv2d(512, hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            - x: (N, C, H, W)\n",
    "            - output: (W, N, C)\n",
    "        \"\"\"\n",
    "\n",
    "        conv = self.features(x)\n",
    "        conv = self.dropout(conv)\n",
    "        conv = self.last_conv_1x1(conv)\n",
    "\n",
    "#        conv = rearrange(conv, 'b d h w -> b d (w h)')\n",
    "        conv = conv.permute(0, 1, 3, 2)\n",
    "        conv = conv.flatten(2)\n",
    "        conv = conv.permute(2, 0, 1)\n",
    "\n",
    "        return conv\n",
    "\n",
    "def vgg11_bn(ss, ks, hidden, pretrained=True, dropout=0.5):\n",
    "    return Vgg('vgg11_bn', ss, ks, hidden, pretrained, dropout)\n",
    "\n",
    "def vgg19_bn(ss, ks, hidden, pretrained=True, dropout=0.5):\n",
    "    return Vgg('vgg19_bn', ss, ks, hidden, pretrained, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cb6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, backbone, **kwargs):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        if backbone == 'vgg11_bn':\n",
    "            self.model = vgg11_bn(**kwargs)\n",
    "        elif backbone == 'vgg19_bn':\n",
    "            self.model = vgg19_bn(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze(self):\n",
    "        for name, param in self.model.features.named_parameters():\n",
    "            if name != 'last_conv_1x1':\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b52302",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea8b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: src_len x batch_size x img_channel\n",
    "        outputs: src_len x batch_size x hid_dim\n",
    "        hidden: batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        embedded = self.dropout(src)\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim,\n",
    "        outputs: batch_size x src_len\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return F.softmax(attention, dim = 1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        inputs: batch_size\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_hidden, decoder_hidden, img_channel, decoder_embedded, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        attn = Attention(encoder_hidden, decoder_hidden)\n",
    "\n",
    "        self.encoder = Encoder(img_channel, encoder_hidden, decoder_hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, decoder_embedded, encoder_hidden, decoder_hidden, dropout, attn)\n",
    "\n",
    "    def forward_encoder(self, src):\n",
    "        \"\"\"\n",
    "        src: timestep x batch_size x channel\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "\n",
    "    def forward_decoder(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: timestep x batch_size\n",
    "        hidden: batch_size x hid_dim\n",
    "        encouder: src_len x batch_size x hid_dim\n",
    "        output: batch_size x 1 x vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        tgt = tgt[-1]\n",
    "        hidden, encoder_outputs = memory\n",
    "        output, hidden, _ = self.decoder(tgt, hidden, encoder_outputs)\n",
    "        output = output.unsqueeze(1)\n",
    "\n",
    "        return output, (hidden, encoder_outputs)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        src: time_step x batch_size\n",
    "        trg: time_step x batch_size\n",
    "        outputs: batch_size x time_step x vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        device = src.device\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        for t in range(trg_len):\n",
    "            input = trg[t]\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "        outputs = outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def expand_memory(self, memory, beam_size):\n",
    "        hidden, encoder_outputs = memory\n",
    "        hidden = hidden.repeat(beam_size, 1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1, beam_size, 1)\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "\n",
    "    def get_memory(self, memory, i):\n",
    "        hidden, encoder_outputs = memory\n",
    "        hidden = hidden[[i]]\n",
    "        encoder_outputs = encoder_outputs[:, [i],:]\n",
    "\n",
    "        return (hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c733a1",
   "metadata": {},
   "source": [
    "# Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d70f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, chars):\n",
    "        self.pad = 0\n",
    "        self.go = 1\n",
    "        self.eos = 2\n",
    "        self.mask_token = 3\n",
    "\n",
    "        self.chars = chars\n",
    "\n",
    "        self.c2i = {c:i+4 for i, c in enumerate(chars)}\n",
    "\n",
    "        self.i2c = {i+4:c for i, c in enumerate(chars)}\n",
    "\n",
    "        self.i2c[0] = '<pad>'\n",
    "        self.i2c[1] = '<sos>'\n",
    "        self.i2c[2] = '<eos>'\n",
    "        self.i2c[3] = '*'\n",
    "\n",
    "    def encode(self, chars):\n",
    "        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        first = 1 if self.go in ids else 0\n",
    "        last = ids.index(self.eos) if self.eos in ids else None\n",
    "        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n",
    "        return sent\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.c2i) + 4\n",
    "\n",
    "    def batch_decode(self, arr):\n",
    "        texts = [self.decode(ids) for ids in arr]\n",
    "        return texts\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb132b2e",
   "metadata": {},
   "source": [
    "# Text-recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c30ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRecognizer(nn.Module):\n",
    "    def __init__(self, vocab_size,\n",
    "                 backbone,\n",
    "                 cnn_args,\n",
    "                 transformer_args, seq_modeling='transformer'):\n",
    "\n",
    "        super(TextRecognizer, self).__init__()\n",
    "\n",
    "        self.cnn = CNN(backbone, **cnn_args)\n",
    "        self.seq_modeling = seq_modeling\n",
    "        self.transformer = Seq2Seq(vocab_size, **transformer_args)\n",
    "\n",
    "\n",
    "    def forward(self, img, tgt_input, tgt_key_padding_mask):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "            - img: (N, C, H, W)\n",
    "            - tgt_input: (T, N)\n",
    "            - tgt_key_padding_mask: (N, T)\n",
    "            - output: b t v\n",
    "        \"\"\"\n",
    "        src = self.cnn(img)\n",
    "        outputs = self.transformer(src, tgt_input)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37739f1",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aee6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def translate(img, model, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    model.eval()\n",
    "    device = img.device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src = model.cnn(img)\n",
    "        memory = model.transformer.forward_encoder(src)\n",
    "\n",
    "        translated_sentence = [[sos_token] * len(img)]\n",
    "        max_length = 0\n",
    "\n",
    "        while max_length <= max_seq_length and not all(np.any(np.asarray(translated_sentence).T == eos_token, axis=1)):\n",
    "            tgt_inp = torch.LongTensor(translated_sentence).to(device)\n",
    "            output, memory = model.transformer.forward_decoder(tgt_inp, memory)\n",
    "            output = output.to('cpu')\n",
    "\n",
    "            values, indices = torch.topk(output, 1)\n",
    "            indices = indices[:, -1, 0]\n",
    "            indices = indices.tolist()\n",
    "\n",
    "            translated_sentence.append(indices)\n",
    "            max_length += 1\n",
    "\n",
    "            del output\n",
    "\n",
    "        translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    vocab = Vocab(config['vocab'])\n",
    "    device = config['device']\n",
    "\n",
    "    model = TextRecognizer(len(vocab),\n",
    "            config['backbone'],\n",
    "            config['cnn'],\n",
    "            config['transformer'],\n",
    "            config['seq_modeling'])\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model, vocab\n",
    "\n",
    "def resize(w, h, expected_height, image_min_width, image_max_width):\n",
    "    new_w = int(expected_height * float(w) / float(h))\n",
    "    round_to = 10\n",
    "    new_w = math.ceil(new_w/round_to)*round_to\n",
    "    new_w = max(new_w, image_min_width)\n",
    "    new_w = min(new_w, image_max_width)\n",
    "\n",
    "    return new_w, expected_height\n",
    "\n",
    "def process_image(image, image_height, image_min_width, image_max_width):\n",
    "    img = image.convert('RGB')\n",
    "\n",
    "    w, h = img.size\n",
    "    new_w, image_height = resize(w, h, image_height, image_min_width, image_max_width)\n",
    "\n",
    "    img = img.resize((new_w, image_height), Image.ANTIALIAS)\n",
    "\n",
    "    img = np.asarray(img).transpose(2,0, 1)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_input(image, image_height, image_min_width, image_max_width):\n",
    "    img = process_image(image, image_height, image_min_width, image_max_width)\n",
    "    img = img[np.newaxis, ...]\n",
    "    img = torch.FloatTensor(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d04db1",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc7b360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justtuananh/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/justtuananh/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "config = Cfg.load_config_from_file('./config/vgg-seq2seq.yml')\n",
    "config['cnn']['pretrained']=False\n",
    "config['device'] = 'cpu'\n",
    "model, vocab = build_model(config)\n",
    "weight_path = 'weight/transformerocr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6272221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "model.load_state_dict(torch.load(weight_path, map_location=torch.device(config['device'])))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0f4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_part(img, save_path, model, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    with torch.no_grad():\n",
    "        src = model.cnn(img)\n",
    "        torch.onnx.export(model.cnn, img, save_path, export_params=True, opset_version=12, do_constant_folding=True, verbose=True, input_names=['img'], output_names=['output'], dynamic_axes={'img': {3: 'lenght'}, 'output': {0: 'channel'}})\n",
    "\n",
    "    return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a673f8",
   "metadata": {},
   "source": [
    "## Convert CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7add5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%img : Float(1, 3, 32, *, strides=[45600, 15200, 475, 1], requires_grad=0, device=cpu),\n",
      "      %model.last_conv_1x1.weight : Float(256, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %model.last_conv_1x1.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_190 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_191 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_193 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_197 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_199 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_200 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_202 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_203 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_205 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_206 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_208 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_209 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_211 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_212 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_214 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_215 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_217 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_218 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_220 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_221 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_223 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_224 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_226 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_227 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_229 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_230 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_232 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_233 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_235 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_236 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/model/features/features.0/Conv_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.0/Conv\"](%img, %onnx::Conv_190, %onnx::Conv_191), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.0 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.2/Relu_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.2/Relu\"](%/model/features/features.0/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.2 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.3/Conv_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.3/Conv\"](%/model/features/features.2/Relu_output_0, %onnx::Conv_193, %onnx::Conv_194), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.3 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.5/Relu_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.5/Relu\"](%/model/features/features.3/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.5 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.6/Constant_output_0 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ], onnx_name=\"/model/features/features.6/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.6 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.6/Pad_output_0 : Float(1, 64, 32, *, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"/model/features/features.6/Pad\"](%/model/features/features.5/Relu_output_0, %/model/features/features.6/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.6 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.6/AveragePool_output_0 : Float(1, 64, 16, *, strides=[242688, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/model/features/features.6/AveragePool\"](%/model/features/features.6/Pad_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.6 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.7/Conv_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.7/Conv\"](%/model/features/features.6/AveragePool_output_0, %onnx::Conv_196, %onnx::Conv_197), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.7 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.9/Relu_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.9/Relu\"](%/model/features/features.7/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.9 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.10/Conv_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.10/Conv\"](%/model/features/features.9/Relu_output_0, %onnx::Conv_199, %onnx::Conv_200), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.10 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.12/Relu_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.12/Relu\"](%/model/features/features.10/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.12 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.13/Constant_output_0 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ], onnx_name=\"/model/features/features.13/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.13 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.13/Pad_output_0 : Float(1, 128, 16, *, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"/model/features/features.13/Pad\"](%/model/features/features.12/Relu_output_0, %/model/features/features.13/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.13 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.13/AveragePool_output_0 : Float(1, 128, 8, *, strides=[120832, 944, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/model/features/features.13/AveragePool\"](%/model/features/features.13/Pad_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.13 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.14/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.14/Conv\"](%/model/features/features.13/AveragePool_output_0, %onnx::Conv_202, %onnx::Conv_203), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.14 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.16/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.16/Relu\"](%/model/features/features.14/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.16 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.17/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.17/Conv\"](%/model/features/features.16/Relu_output_0, %onnx::Conv_205, %onnx::Conv_206), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.17 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.19/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.19/Relu\"](%/model/features/features.17/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.19 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.20/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.20/Conv\"](%/model/features/features.19/Relu_output_0, %onnx::Conv_208, %onnx::Conv_209), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.20 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.22/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.22/Relu\"](%/model/features/features.20/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.22 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.23/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.23/Conv\"](%/model/features/features.22/Relu_output_0, %onnx::Conv_211, %onnx::Conv_212), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.23 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.25/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.25/Relu\"](%/model/features/features.23/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.25 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.26/Constant_output_0 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ], onnx_name=\"/model/features/features.26/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.26 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.26/Pad_output_0 : Float(1, 256, 8, *, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"/model/features/features.26/Pad\"](%/model/features/features.25/Relu_output_0, %/model/features/features.26/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.26 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.26/AveragePool_output_0 : Float(1, 256, 4, *, strides=[120832, 472, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1], onnx_name=\"/model/features/features.26/AveragePool\"](%/model/features/features.26/Pad_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.26 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.27/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.27/Conv\"](%/model/features/features.26/AveragePool_output_0, %onnx::Conv_214, %onnx::Conv_215), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.27 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.29/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.29/Relu\"](%/model/features/features.27/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.29 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.30/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.30/Conv\"](%/model/features/features.29/Relu_output_0, %onnx::Conv_217, %onnx::Conv_218), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.30 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.32/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.32/Relu\"](%/model/features/features.30/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.32 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.33/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.33/Conv\"](%/model/features/features.32/Relu_output_0, %onnx::Conv_220, %onnx::Conv_221), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.33 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.35/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.35/Relu\"](%/model/features/features.33/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.35 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.36/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.36/Conv\"](%/model/features/features.35/Relu_output_0, %onnx::Conv_223, %onnx::Conv_224), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.36 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.38/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.38/Relu\"](%/model/features/features.36/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.38 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.39/Constant_output_0 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ], onnx_name=\"/model/features/features.39/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.39 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.39/Pad_output_0 : Float(1, 512, 4, *, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"/model/features/features.39/Pad\"](%/model/features/features.38/Relu_output_0, %/model/features/features.39/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.39 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.39/AveragePool_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1], onnx_name=\"/model/features/features.39/AveragePool\"](%/model/features/features.39/Pad_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.39 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.40/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.40/Conv\"](%/model/features/features.39/AveragePool_output_0, %onnx::Conv_226, %onnx::Conv_227), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.40 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.42/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.42/Relu\"](%/model/features/features.40/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.42 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.43/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.43/Conv\"](%/model/features/features.42/Relu_output_0, %onnx::Conv_229, %onnx::Conv_230), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.43 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.45/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.45/Relu\"](%/model/features/features.43/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.45 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.46/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.46/Conv\"](%/model/features/features.45/Relu_output_0, %onnx::Conv_232, %onnx::Conv_233), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.46 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.48/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.48/Relu\"](%/model/features/features.46/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.48 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.49/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.49/Conv\"](%/model/features/features.48/Relu_output_0, %onnx::Conv_235, %onnx::Conv_236), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.49 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/features/features.51/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.51/Relu\"](%/model/features/features.49/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.51 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/features/features.52/Constant_output_0 : Long(8, strides=[1], device=cpu) = onnx::Constant[value= 0  0  0  0  0  0  0  0 [ CPULongType{8} ], onnx_name=\"/model/features/features.52/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.52 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.52/Pad_output_0 : Float(1, 512, 2, *, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"/model/features/features.52/Pad\"](%/model/features/features.51/Relu_output_0, %/model/features/features.52/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.52 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/features/features.52/AveragePool_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/features/features.52/AveragePool\"](%/model/features/features.52/Pad_output_0), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.52 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/pooling.py:639:0\n",
      "  %/model/last_conv_1x1/Conv_output_0 : Float(1, 256, 2, *, strides=[60416, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/last_conv_1x1/Conv\"](%/model/features/features.52/AveragePool_output_0, %model.last_conv_1x1.weight, %model.last_conv_1x1.bias), scope: __main__.CNN::/__main__.Vgg::model/torch.nn.modules.conv.Conv2d::last_conv_1x1 # /home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/Transpose_output_0 : Float(1, 256, *, 2, strides=[60416, 236, 1, 118], requires_grad=0, device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name=\"/model/Transpose\"](%/model/last_conv_1x1/Conv_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:33:0\n",
      "  %/model/Shape_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/model/Shape\"](%/model/Transpose_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Constant_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/model/Constant\"](), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Constant_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/model/Constant_1\"](), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Constant_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={2}, onnx_name=\"/model/Constant_2\"](), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Slice_output_0 : Long(2, strides=[1], device=cpu) = onnx::Slice[onnx_name=\"/model/Slice\"](%/model/Shape_output_0, %/model/Constant_1_output_0, %/model/Constant_2_output_0, %/model/Constant_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Constant_3_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/model/Constant_3\"](), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Concat_output_0 : Long(3, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/model/Concat\"](%/model/Slice_output_0, %/model/Constant_3_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %/model/Reshape_output_0 : Float(*, *, *, strides=[60416, 236, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"/model/Reshape\"](%/model/Transpose_output_0, %/model/Concat_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:34:0\n",
      "  %output : Float(*, *, *, strides=[1, 60416, 236], requires_grad=0, device=cpu) = onnx::Transpose[perm=[2, 0, 1], onnx_name=\"/model/Transpose_1\"](%/model/Reshape_output_0), scope: __main__.CNN::/__main__.Vgg::model # /tmp/ipykernel_727994/1703846863.py:35:0\n",
      "  return (%output)\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 32, 475)\n",
    "src = convert_cnn_part(img, './weight/cnn.onnx', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342ea7d",
   "metadata": {},
   "source": [
    "## Export encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc241f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder_part(model, src, save_path): \n",
    "    encoder_outputs, hidden = model.transformer.encoder(src) \n",
    "    torch.onnx.export(model.transformer.encoder, src, save_path, export_params=True, opset_version=11, do_constant_folding=True, input_names=['src'], output_names=['encoder_outputs', 'hidden'], dynamic_axes={'src':{0: \"channel_input\"}, 'encoder_outputs': {0: 'channel_output'}}) \n",
    "    return hidden, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d78c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justtuananh/anaconda3/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:4476: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hidden, encoder_outputs = convert_encoder_part(model, src, './weight/encoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88231da9",
   "metadata": {},
   "source": [
    "## Export decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc7873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decoder_part(model, tgt, hidden, encoder_outputs, save_path):\n",
    "    tgt = tgt[-1]\n",
    "    \n",
    "    torch.onnx.export(model.transformer.decoder,\n",
    "        (tgt, hidden, encoder_outputs),\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['tgt', 'hidden', 'encoder_outputs'],\n",
    "        output_names=['output', 'hidden_out', 'last'],\n",
    "        dynamic_axes={'encoder_outputs':{0: \"channel_input\"},\n",
    "                    'last': {0: 'channel_output'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c205ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = img.device\n",
    "tgt = torch.LongTensor([[1] * len(img)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24459f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_727994/871725737.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (output == hidden).all()\n"
     ]
    }
   ],
   "source": [
    "convert_decoder_part(model, tgt, hidden, encoder_outputs, './weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64dae2",
   "metadata": {},
   "source": [
    "## Load onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff2be87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5387c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = onnx.load('./weight/cnn.onnx')\n",
    "decoder = onnx.load('./weight/encoder.onnx')\n",
    "encoder = onnx.load('./weight/decoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3fd06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(cnn)\n",
    "onnx.checker.check_model(decoder)\n",
    "onnx.checker.check_model(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eeb96b",
   "metadata": {},
   "source": [
    "## Inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8847167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('demo.png')\n",
    "img = process_input(img, config['dataset']['image_height'], \n",
    "                config['dataset']['image_min_width'], config['dataset']['image_max_width'])  \n",
    "img = img.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b30ed4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Người thương'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = translate(img, model)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbce504",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime's Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06fbb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a6b1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_session = onnxruntime.InferenceSession(\"./weight/cnn.onnx\")\n",
    "encoder_session = onnxruntime.InferenceSession(\"./weight/encoder.onnx\")\n",
    "decoder_session = onnxruntime.InferenceSession(\"./weight/decoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "796cd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_onnx(img, session, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    cnn_session, encoder_session, decoder_session = session\n",
    "    \n",
    "    # create cnn input\n",
    "    cnn_input = {cnn_session.get_inputs()[0].name: img}\n",
    "    src = cnn_session.run(None, cnn_input)\n",
    "    \n",
    "    # create encoder input\n",
    "    encoder_input = {encoder_session.get_inputs()[0].name: src[0]}\n",
    "    encoder_outputs, hidden = encoder_session.run(None, encoder_input)\n",
    "    translated_sentence = [[sos_token] * len(img)]\n",
    "    max_length = 0\n",
    "\n",
    "    while max_length <= max_seq_length and not all(\n",
    "        np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n",
    "    ):\n",
    "        tgt_inp = translated_sentence\n",
    "        decoder_input = {decoder_session.get_inputs()[0].name: tgt_inp[-1], decoder_session.get_inputs()[1].name: hidden, decoder_session.get_inputs()[2].name: encoder_outputs}\n",
    "\n",
    "        output, hidden, _ = decoder_session.run(None, decoder_input)\n",
    "        output = np.expand_dims(output, axis=1)\n",
    "        output = torch.Tensor(output)\n",
    "\n",
    "        values, indices = torch.topk(output, 1)\n",
    "        indices = indices[:, -1, 0]\n",
    "        indices = indices.tolist()\n",
    "\n",
    "        translated_sentence.append(indices)\n",
    "        max_length += 1\n",
    "\n",
    "        del output\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10b821fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_727994/2456424739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcnn_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_727994/3009350291.py\u001b[0m in \u001b[0;36mtranslate_onnx\u001b[0;34m(img, session, max_seq_length, sos_token, eos_token)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# create cnn input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcnn_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# create encoder input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))"
     ]
    }
   ],
   "source": [
    "\n",
    "session = (cnn_session, encoder_session, decoder_session)\n",
    "s = translate_onnx(np.array(img), session)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
